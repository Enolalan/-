#+TITLE: elasticsearch
#+AUTHOR: lujun9972
#+TAGS: .
#+DATE: [2017-05-25 四 15:10]
#+LANGUAGE:  zh-CN
#+OPTIONS:  H:6 num:nil toc:t \n:nil ::t |:t ^:nil -:nil f:t *:t <:nil

* document的核心元数据

** _index
文档所属的索引. 我们一般把类似的数据(document的大部分field相同)放在一个索引中,非类似的数据放在不同索引中.

索引只能由小写字母组成

** _type   
document所属类别. 可以用大小写字母

** _id  
指定文档id,可以手工指定,也可以由es自动生成.

1. 手工指定id
   #+BEGIN_SRC es
     PUT /index/type/id
   #+END_SRC

   一般在从其他系统导入数据到es中时使用这种方式,id为原数据的主键

2. 自动生成id
   #+BEGIN_SRC es
     POST /index/type
   #+END_SRC

** _version
版本号,从1开始累加,是乐观锁的基础
** _source元数据
文档内容

可以使用 =GET /index/type/1?_source=field1,field2= 语法来定制返回_source中的数据
   
* document的创建,更新与删除
** 创建/全量替换
#+BEGIN_SRC es
  PUT /index/type/$id
  {
    内容
  }
#+END_SRC
** 强制创建
#+BEGIN_SRC es
  PUT /index/type/$id/_create
  {
    内容
  }
#+END_SRC
这样当$id已经存在时,不会替换原document,而是会提示错误
** 带版本号更新
#+BEGIN_SRC es
  PUT /index/type/$id?version=$version
  {
    内容
  }
#+END_SRC
这样只有当es中该document的版本 *等于* $version时才会更新该document

但也可以extern_version
#+BEGIN_SRC es
  PUT /index/type/$id?version=$version&version_type=external
  {
    内容
  }
#+END_SRC
这时只要$version比es中的版本 *大* 就能更新了.
** 部分更新
#+BEGIN_SRC es
  POST /index/type/$id/_update
  {
    "doc":{
      要修改的几个field
    }
  }
#+END_SRC

partial update也支持指定更新的document version
#+BEGIN_SRC es
  POST /index/type/$id/_update?_version=$version
  {
    "doc":{
      要修改的几个field
    }
  }
#+END_SRC


实际上partial update还常用到参数 =retry_on_conflict= 用来指定更新冲突时重复更新的次数.
#+BEGIN_SRC es
  POST /index/type/$id/_update?retry_on_conflict=$times
  {
    "doc":{
      要修改的几个field
    }
  }
#+END_SRC


** 删除
#+BEGIN_SRC es
  DELETE /index/type/$id
#+END_SRC
* 索引
** 创建索引
#+BEGIN_SRC es
  PUT /index
  {
    "settings":{...},
    "mappings":{
      "type_one":{......},
      "type_two":"{....}",
      .....
    }
  }
#+END_SRC
** 修改索引
#+BEGIN_SRC es
  PUT /index/_settings
  {
    ...
  }
#+END_SRC
** 删除索引
#+BEGIN_SRC es
  DELETE /index
  DELETE /index1,index2
  DELETE /index*
  DELETE /_all
#+END_SRC
** mapping
*** 核心的数据类型
+ string / text
+ long
+ double
+ boolean
+ date
*** 创建mapping
注意: 只能创建index时手工创建mapping,或者新增field mapping,而 *无法修改mapping*

+ 创建index时手工创建mapping
  #+BEGIN_SRC es
    PUT /index/
    {
      "mappings":{
        "$type":{
          "properties":{
            "$field1" :{
              "type":"string",
              "analyzer":"english"
            },
            "$field2" :{
              "type":"data"
            },
            "$filed3":{
              "type":"long"
            },
            "$field4":{
              "type":"text",
              "index":"not_analyzed"
            }
          }
        }
      }
    }
  #+END_SRC

+ 新增field mapping
  #+BEGIN_SRC es
    PUT /index/_mapping/type
    {
      "properties":{
        "field5":{
          "type":"string"
        },
        "field6":{
          "type":"boolean"
        }
      }
    }
  #+END_SRC
*** 获取mapping信息
1. 获取指定index下type的mapping
   #+BEGIN_SRC es
     GET /index/_mapping/type
   #+END_SRC
* analyze
它是将文本块转换为有区别的、规范化的 token 的一个过程，目的是为了（a）创建倒排索引以及（b）查询倒排索引。 
#+BEGIN_EXAMPLE
  GET /my_index/_analyze
  {
    "field": "productID",
    "text": "XHDK-A-1293-#fJ3"
  }
#+END_EXAMPLE

测试"XHDK-A-1293-#ffJ3"在域"productID"上的分词效果
** 测试分词器
#+BEGIN_SRC es
  GET /_analyze
  {
    "analyzer" : "standard",
    "text": "Text to analyze"
  }
#+END_SRC

* 搜索
** 验证搜索合法性并找出失败原因
#+BEGIN_SRC es
  GET /index/type/_validate/query?explain
  {
    搜索DSL
  }
#+END_SRC
** 超时机制
默认elasticsearch不会超时,但是可以通过在 _update api后接 =_time=$带单位的时间= 来设置超时
#+BEGIN_SRC es
  GET /_search?_timeout=10ms
#+END_SRC
** mult-index与multi-type搜索
+ /_search :: 搜索所有索引和类型下的所有数据
+ /index1/_search :: 搜索指定index1下的所有类型的数据
+ /_index1,index2/_search :: 同时搜索两个index下的数据
+ /index*/_search :: 通过通配符去匹配多个索引
+ /index1/type1/_index :: 搜索一个index1下type2类的数据
+ /index1/type1,type2/_index :: 搜索一个索引index1下多个type的数据
+ /index1,index2/type1,typ2/_index :: 指定多个索引下多个类型的数据
+ /_all/type1,type2/_search :: 就是/*/type1,type2/_search
** 分页搜索
size参数和from参数
#+BEGIN_SRC es
  /_search?size=10&from=0
#+END_SRC
** 批量查询mget
#+BEGIN_SRC es
  GET /_mget
  {
    "doc":[
      {"_index":$index,
       "_type":$type,
       "_id":$id
      }
      {"_index":$index,
       "_type":$type,
       "_id":$id
      }
      {"_index":$index,
       "_type":$type,
       "_id":$id
      }
    ]
  }
#+END_SRC
若只想查询某个index下的内容,则可以将$index放入URL中,在body中省略"_index"
#+BEGIN_SRC es
  GET /index/_mget
  {
    "doc":[
      {"_type":$type,
       "_id":$id
      }
      {"_type":$type,
       "_id":$id
      }
      {"_type":$type,
       "_id":$id
      }
    ]
  }
#+END_SRC
你也可以同时省略index和type
#+BEGIN_SRC es
  GET /index/type/_mget
  {
    "ids":[$id,$id,$id]
  }
#+END_SRC

** 精确搜索
当进行精确值查找时， 我们会使用过滤器（filters）。过滤器很重要，因为它们执行速度非常快，不会计算相关度（直接跳过了整个评分阶段）而且很容易被缓存。

*** term查询
我们首先来看最为常用的 term 查询， 可以用它处理数字（numbers）、布尔值（Booleans）、日期（dates）以及文本（text）。
term 查询会查找我们指定的精确值。作为其本身， term 查询是简单的。它接受一个字段名以及我们希望查找的数值：

#+BEGIN_EXAMPLE
  {
      "term" : {
          "price" : 20
      }
  }
#+END_EXAMPLE

通常当查找一个精确值的时候，我们不希望对查询进行评分计算。只希望对文档进行包括或排除的计算，所以我们会使用 constant_score 查询以非评分模式来执行 term 查询并以一作为统一评分。

例如：
#+BEGIN_EXAMPLE
  GET /my_store/products/_search
  {
      "query" : {
          "constant_score" : { 
              "filter" : {
                  "term" : { 
                      "price" : 20
                  }
              }
          }
      }
  }
#+END_EXAMPLE

相当于

#+BEGIN_SRC sql
  SELECT *
  FROM   products
  WHERE  price = 20
#+END_SRC

*** terms查询
terms查询可以一次查找多个精确指，它几乎与 term 的使用方式一模一样，与指定单个价格不同，我们只要将 term 字段的值改为数组即可：

#+BEGIN_EXAMPLE
  {
      "terms" : {
          "price" : [20, 30]
      }
  }
#+END_EXAMPLE

例如
#+BEGIN_EXAMPLE
  GET /my_store/products/_search
  {
      "query" : {
          "constant_score" : {
              "filter" : {
                  "terms" : { 
                      "price" : [20, 30]
                  }
              }
          }
      }
  }
#+END_EXAMPLE

类似于

#+BEGIN_SRC sql
  SELECT *
  FROM   products
  WHERE  price = in (20,30)
#+END_SRC

*term 和 terms 是 必须包含（must contain） 操作，而不是 必须精确相等（must equal exactly）*

如果我们有一个 term（词项）过滤器 { "term" : { "tags" : "search" } } ，它会与以下两个文档 同时 匹配：

#+BEGIN_EXAMPLE
  { "tags" : ["search"] }
  { "tags" : ["search", "open_source"] } 
#+END_EXAMPLE

尽管第二个文档包含除 search 以外的其他词，它还是被匹配并作为结果返回。 

*** range查询
可以用它来查找处于某个范围内的文档：

**** 数字范围
#+BEGIN_EXAMPLE
  "range" : {
      "price" : {
          "gte" : 20,
          "lte" : 40
      }
  }
#+END_EXAMPLE

+ gt :: > 大于（greater than）
+ lt :: < 小于（less than）
+ gte :: >= 大于或等于（greater than or equal to）
+ lte :: <= 小于或等于（less than or equal to） 
         
例如
#+BEGIN_EXAMPLE
  GET /my_store/products/_search
  {
      "query" : {
          "constant_score" : {
              "filter" : {
                  "range" : {
                      "price" : {
                          "gte" : 20,
                          "lt"  : 40
                      }
                  }
              }
          }
      }
  }
#+END_EXAMPLE

相当于

#+BEGIN_SRC sql
  select * from products where price >=20 and price<40
#+END_SRC

**** 日期范围
ange 查询同样可以应用在日期字段上：

#+BEGIN_EXAMPLE
  "range" : {
      "timestamp" : {
          "gt" : "2014-01-01 00:00:00",
          "lt" : "2014-01-07 00:00:00"
      }
  }
#+END_EXAMPLE

当使用它处理日期字段时， range 查询支持对 日期计算（date math） 进行操作，比方说，如果我们想查找时间戳在过去一小时内的所有文档：

#+BEGIN_EXAMPLE
  "range" : {
      "timestamp" : {
          "gt" : "now-1h"
      }
  }
#+END_EXAMPLE
日期计算还可以被应用到某个具体的时间，并非只能是一个像 now 这样的占位符。只要在某个日期后加上一个双管符号 (||) 并紧跟一个日期数学表达式就能做到：

#+BEGIN_EXAMPLE
  "range" : {
      "timestamp" : {
          "gt" : "2014-01-01 00:00:00",
          "lt" : "2014-01-01 00:00:00||+1M" 
      }
  }
#+END_EXAMPLE

**** 字符串范围
range 查询同样可以处理字符串字段
#+BEGIN_EXAMPLE
  "range" : {
      "title" : {
          "gte" : "a",
          "lt" :  "b"
      }
  }
#+END_EXAMPLE

*** exists查询
这个查询会返回那些在指定字段有任何值的文档

例如：
#+BEGIN_EXAMPLE
  GET /my_index/posts/_search
  {
      "query" : {
          "constant_score" : {
              "filter" : {
                  "exists" : { "field" : "tags" }
              }
          }
      }
  }
#+END_EXAMPLE

类似于

#+BEGIN_SRC sql
  select * from posts where tags is not null
#+END_SRC


*** missing查询
missing 查询本质上与 exists 恰好相反： 它返回某个特定 _无_ 值字段的文档:

例如：
#+BEGIN_EXAMPLE
  GET /my_index/posts/_search
  {
      "query" : {
          "constant_score" : {
              "filter": {
                  "missing" : { "field" : "tags" }
              }
          }
      }
  }
#+END_EXAMPLE

类似于

#+BEGIN_SRC sql
  select * from posts where tags is null
#+END_SRC

注意： *我们不能判断是一个字段有明确的null值，还是根本没有这个字段*

*** 布尔过滤器
一个 bool 过滤器由三部分组成：
#+BEGIN_EXAMPLE
  {
     "bool" : {
        "must" :     [],
        "should" :   [],
        "must_not" : [],
     }
  }
#+END_EXAMPLE

+ must :: 所有的语句都 必须（must） 匹配，与 AND 等价。 
+ must_not :: 所有的语句都 不能（must not） 匹配，与 NOT 等价。 
+ should :: 至少有一个语句要匹配，与 OR 等价。 
            
例如：
#+BEGIN_EXAMPLE
  GET /my_store/products/_search
  {
     "query" : {
        "filtered" : { 
           "filter" : {
              "bool" : {
                "should" : [
                   { "term" : {"price" : 20}}, 
                   { "term" : {"productID" : "XHDK-A-1293-#fJ3"}} 
                ],
                "must_not" : {
                   "term" : {"price" : 30} 
                }
             }
           }
        }
     }
  }
#+END_EXAMPLE

相当于：

#+BEGIN_SRC sql
  SELECT *
  FROM   products
  WHERE  (price = 20 OR productID = "XHDK-A-1293-#fJ3")
  AND  (price != 30)
#+END_SRC

*** 嵌套布尔过滤器
尽管 bool 是一个复合的过滤器，可以接受多个子过滤器，需要注意的是 bool 过滤器本身仍然还只是一个过滤器。
这意味着我们可以将一个 bool 过滤器置于其他 bool 过滤器内部，这为我们提供了对任意复杂布尔逻辑进行处理的能力。

例如
#+BEGIN_EXAMPLE
  GET /my_store/products/_search
  {
     "query" : {
        "filtered" : {
           "filter" : {
              "bool" : {
                "should" : [
                  { "term" : {"productID" : "KDKE-B-9947-#kL5"}}, 
                  { "bool" : { 
                    "must" : [
                      { "term" : {"productID" : "JODL-X-1937-#pV7"}}, 
                      { "term" : {"price" : 30}} 
                    ]
                  }}
                ]
             }
           }
        }
     }
  }
#+END_EXAMPLE

类似于

#+BEGIN_SRC sql
  SELECT *
  FROM   products
  WHERE  productID      = "KDKE-B-9947-#kL5"
  OR (     productID = "JODL-X-1937-#pV7"
  AND price     = 30 )
#+END_SRC

** search api基本语法
#+BEGIN_SRC es
  GET /index/_search
  {
    条件
  }
#+END_SRC


#+BEGIN_SRC es
  GET /index1,index2/type1,type2/_search
  {
    条件
  }
#+END_SRC

#+BEGIN_SRC es
  GET /index1,index2/type1,type2/_search
  {
    "from":0,
    "size":10,
    条件
  }
#+END_SRC

** Query DSL的基本语法
#+BEGIN_SRC es
  GET /_search
  {
    "query":{
      "match_all":{}
    }
  }
#+END_SRC

#+BEGIN_SRC es
  {
    "query":{
      QUERY_NAME:{
        ARGUMENT:VALUE,
        ARGUMENT:VALUE,...
      }
    }
  }

#+END_SRC

#+BEGIN_SRC es
  {
    "query":{
      QUERY_NAME:{
        FIELD_NAME:{
          ARGUMENT:VALUE,
          ARGUMENT:VALUE,...
        }
      }
    }
  }

#+END_SRC

** 全文查询
像 match 或 query_string 这样的查询是高层查询，它们了解字段映射的信息：

+ 如果查询 日期（date） 或 整数（integer） 字段，它们会将查询字符串分别作为日期或整数对待。
+ 如果查询一个（ not_analyzed ）未分析的精确值字符串字段， 它们会将整个查询字符串作为单个词项对待。
+ 但如果要查询一个（ analyzed ）已分析的全文字段， 它们会先将查询字符串传递到一个合适的分析器，然后生成一个供查询的词项列表。

*** match查询
match查询既能处理全文字段，又能处理精确字段。match 查询主要的应用场景就是进行全文搜索。

#+BEGIN_EXAMPLE
  GET /my_index/my_type/_search
  {
      "query": {
          "match": {
              "title": "QUICK! DOG"
          }
      }
  }
#+END_EXAMPLE

Elasticsearch 执行上面这个 match 查询的步骤是：

+ 检查字段类型 。

  标题 title 字段是一个 string 类型（ analyzed ）已分析的全文字段，这意味着查询字符串本身也应该被分析。

+ 分析查询字符串 。
  
  将查询的字符串 "QUICK! DOG" 传入标准分析器中，输出的结果是["brown","dog"]。因为 match 查询必须查找两个词（ ["brown","dog"] ），它在内部实际上先执行两次 term 查询，然后将两次查询的结果合并作为最终结果输出。

+ 为每个文档评分 。

  用 term 查询计算每个文档相关度评分 _score ，这是种将 词频（term frequency，即词 quick 在相关文档的 title 字段中出现的频率）和反向文档频率（inverse document frequency，即词 quick 在所有文档的 title 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。参见 相关性的介绍 。

  
任何文档只要字段里包含 指定词项中的至少一个词 就能匹配，被匹配的词项越多，文档就越相关.
那么，如果我们只想搜索包含 所有 词项的文档，也就是说，不去匹配 brown OR dog ，而通过匹配 brown AND dog 找到所有文档该怎么办呢？

match 查询还可以接受 *operator* 操作符作为输入参数，默认情况下该操作符是 or 。我们可以将它修改成 and 让所有指定词项都必须匹配：
#+BEGIN_EXAMPLE
  GET /my_index/my_type/_search
  {
      "query": {
          "match": {
              "title": {      
                  "query":    "BROWN DOG!",
                  "operator": "and"
              }
          }
      }
  }
#+END_EXAMPLE

match 查询支持 *minimum_should_match* 最小匹配参数， 这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量：
#+BEGIN_EXAMPLE
  GET /my_index/my_type/_search
  {
    "query": {
      "match": {
        "title": {
          "query":                "quick brown dog",
          "minimum_should_match": "75%"
        }
      }
    }
  }
#+END_EXAMPLE

match查询还能使用 =fuzziness= 参数来自动纠正query中的单词错误
#+BEGIN_SRC es
  GET /my_index/my_type/_search
  {
    "query": {
      "match": {
        "title": {
          "query":                "quick brown dog",
          "minimum_should_match": "75%",
          "fuzziness":"AUTO"
        }
      }
    }
  }
#+END_SRC

*** match_phrase 查询(短语搜索)
短语搜索不会对搜索字段进行拆分
  GET /my_index/my_type/_search
  {
      "query": {
          "match_phrase": {
              "title": "QUICK! DOG"
          }
      }
  }

match_phrase 短语匹配查询，它匹配相对顺序一致的所有指定词语，

短语查询还支持slop参数,用于指定短语中的各个词语之间所允许的最大间隔数(QUICK和DOG移动的次数)
#+BEGIN_SRC es
  GET /index/type/_search
  {
    "query":{
      "match_phrase":{
        "title":{
          "query":"QUICK! DOG",
          "slop":3
        }
      }
    }
  }
#+END_SRC


**** 与match结合平衡召回率和精准度
可以使用bool组合match query和match_phrase query一起
#+BEGIN_SRC es
  GET /index/type/_search
  {
    "query":{
      "bool":{
        "must":{
          "match":{
            "title":{
              "query":"java spark",
              "minimum_should_match":"50%"
            }
          }
        },
        "should":{
          "match_phrase":{
            "title":{
              "query":"java spark",
              "slop":50
            }
          }
        }
      }
    }
  }
#+END_SRC

**** 优化性能
一般来说,match_phrase的耗时是match的10倍

为了提高效率,可以先使用match搜索出要显示的内容,然后再通过rescore机制对这些内容根据match phrase搜索来重新打分.
#+BEGIN_SRC es
  GET /index/type/_search
  {
    "query": {
      "match":{
        "title":{
          "query": "java spark"
        }
      }
    },
    "rescore":{
      "window_size": 50,
      "query":{
        "rescore_query":{
          "match_phrase":{
            "title":{
              "query":"java spark",
              "slop":50
            }
          }
        }
      }
    }
  }
#+END_SRC

*** 输入时查询
**** 使用match_phrase_prefix实现search-time搜索推荐
对于查询时的输入即搜索，可以使用 match_phrase 的一种特殊形式， match_phrase_prefix 查询：

其原理与match_phrase类似,但是会把最后一个term作为前缀来搜索.

与 match_phrase 一样，它也可以接受 slop 参数（参照 slop ）让相对词序位置不那么严格：

可以通过设置 max_expansions 参数来限制前缀扩展的影响， 一个合理的值是可能是 50 ：
#+BEGIN_EXAMPLE
  {
      "match_phrase_prefix" : {
          "brand" : {
              "query": "walker johnnie bl", 
              "slop":  10
              "max_expansions": 50
          }
      }
  }
#+END_EXAMPLE
**** 使用ngram分词机制实现index-time搜索推荐
使用edge ngram将每个单词都进一步分词切分,用切分后的ngram来实现前缀搜索推荐功能

比如一个单词"quick"会被分解为q,qu,qui,quic,quick

#+BEGIN_SRC es
  POST /index
  {
    "settings":{
      "filter":{
        "$自定义filter":{
          "type":"edge_ngram",
          "min_gram":1,
          "max_gram":20
        }
      },
      "analyzer":{
        "$自定义filter":{
          "type":"custom",
          "tokenizer":"standard",
          "filter":[
            "lowercase",
            "$自定义filter"
          ]
        }
      }
    }
  }
#+END_SRC

自定义好analyzer后,就可以在定义field时使用了
#+BEGIN_SRC es
  PUT /index/_mapping/type
  {
    "properties":{
      "title":{
        "type":"string",
        "analyzer":"$自定义的autocomplete_analyzer",
        "search_analyzer":"standard"
      }
    }
  }
#+END_SRC

但是请注意, *这里搜索的analyzer依然是"standard"*
*** bool查询
bool查询与bool过滤器功能类似，但是它除了决定一个文档是否应该被包括在结果中，还会计算文档的相关程度 。

与过滤器一样， bool 查询也可以接受 must 、 must_not 和 should 参数下的多个查询语句。比如：
#+BEGIN_EXAMPLE
  GET /my_index/my_type/_search
  {
    "query": {
      "bool": {
        "must":     { "match": { "title": "quick" }},
        "must_not": { "match": { "title": "lazy"  }},
        "should": [
                    { "match": { "title": "brown" }},
                    { "match": { "title": "dog"   }}
        ]
      }
    }
  }
#+END_EXAMPLE

*must,must_not,should 后面可以用[]将多个匹配条件括起来,若匹配条件只有一个,也可以省略这个[]*.

bool查询与bool过滤器的区别就在于两个 *should* 语句的意义，也就是说：一个文档不必包含 brown 或 dog 这两个词项，但如果一旦包含，我们就认为它们 更相关.

所有 must 语句必须匹配，所有 must_not 语句都必须不匹配，没有 should 语句是必须匹配的，
只有一个例外：*那就是当没有 must 语句的时候，至少有一个 should 语句必须匹配*

我们可以通过 *minimum_should_match* 参数控制需要匹配的 should 语句的数量， 
它既可以是一个绝对的数字，又可以是个百分比：

#+BEGIN_EXAMPLE
  GET /my_index/my_type/_search
  {
    "query": {
      "bool": {
        "should": [
          { "match": { "title": "brown" }},
          { "match": { "title": "fox"   }},
          { "match": { "title": "dog"   }}
        ],
        "minimum_should_match": 2 
      }
    }
  }
#+END_EXAMPLE

*** 修改比重
**** 使用boost在查询时修改权重
我们可以通过指定 boost 来控制任何查询语句的相对的权重， boost 的默认值为 1 ，大于 1 会提升一个语句的相对权重。例如这样：

#+BEGIN_EXAMPLE
  GET /_search
  {
      "query": {
          "bool": {
              "must": {
                  "match": {  
                      "content": {
                          "query":    "full text search",
                          "operator": "and"
                      }
                  }
              },
              "should": [
                  { "match": {
                      "content": {
                          "query": "Elasticsearch",
                          "boost": 3 
                      }
                  }},
                  { "match": {
                      "content": {
                          "query": "Lucene",
                          "boost": 2 
                      }
                  }}
              ]
          }
      }
  }
#+END_EXAMPLE

任意类型的查询都能接受 boost 参数,不过,将 boost 设置为 2 ，并不代表最终的评分 _score 是原值的两倍；实际的权重值会经过归一化和一些其他内部优化过程。

在实际应用中，无法通过简单的公式得出某个特定查询语句的 “正确” 权重提升值，只能通过不断尝试获得。
**** 提升索引的权重
当在多个索引中搜索时， 可以使用参数 indices_boost 来提升整个索引的权重
#+BEGIN_EXAMPLE
  GET /docs_2014_*/_search 
  {
    "indices_boost": { 
      "docs_2014_10": 3,
      "docs_2014_09": 2
    },
    "query": {
      "match": {
        "text": "quick brown fox"
      }
    }
  }
#+END_EXAMPLE

*** dis_max查询(best_fields策略)
dis_max查询指的是： 将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回 ：
例如：
#+BEGIN_EXAMPLE
  {
      "query": {
          "dis_max": {
              "queries": [
                  { "match": { "title": "Brown fox" }},
                  { "match": { "body":  "Brown fox" }}
              ]
          }
      }
  }
#+END_EXAMPLE

相比之下，bool查询中，会对should中的所有查询的匹配度进行平均计算来作为整个文档的匹配度。

**** tie_breaker参数
不过通过指定 =tie_breaker= 这个参数将其他匹配语句的评分也考虑其中：
#+BEGIN_EXAMPLE
  {
      "query": {
          "dis_max": {
              "queries": [
                  { "match": { "title": "Quick pets" }},
                  { "match": { "body":  "Quick pets" }}
              ],
              "tie_breaker": 0.3
          }
      }
  }
#+END_EXAMPLE
tie_breaker 参数提供了一种 dis_max 和 bool 之间的折中选择，它的评分方式如下：

+ 获得最佳匹配语句的评分 _score 。
+ 将其他匹配语句的评分结果与 tie_breaker 相乘。
+ 对以上评分求和并规范化。

有了 =tie_breaker= ，会考虑所有匹配语句，但最佳匹配语句依然占最终结果里的很大一部分。
#+BEGIN_QUOTE
tie_breaker 可以是 0 到 1 之间的浮点数，
其中 0 代表使用 dis_max 最佳匹配语句的普通逻辑， 
1 表示所有匹配语句同等重要。
最佳的精确值需要根据数据与查询调试得出，
但是合理值应该与零接近（处于 0.1 - 0.4 之间），这样就不会颠覆 dis_max 最佳匹配性质的根本。
#+END_QUOTE
*** multi_match查询
multi_match查询可以针对多个field进行查询，并指定查询类型是 best_fields,most_fields(优先返回最多匹配项的文档)还是cross_fields

它可以用来简化bool查询中should子句中的多个match查询。
例如：
#+BEGIN_EXAMPLE
  {
      "multi_match": {
          "query":                "Quick brown fox",
          "type":                 "best_fields", 
          "fields":               [ "title", "body" ],
          "tie_breaker":          0.3,
          "minimum_should_match": "30%" 
      }
  }
#+END_EXAMPLE

+ best_fields 类型是默认值，可以不指定。

+ 像 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。

+ 字段名称可以用模糊匹配的方式给出：任何与模糊模式正则匹配的字段都会被包括在搜索条件中.例如：
  #+BEGIN_EXAMPLE
    {
        "multi_match": {
            "query":  "Quick brown fox",
            "fields": "*_title"
        }
    }
  #+END_EXAMPLE

+ 可以使用 ^ 字符语法为单个字段提升权重，在字段名称的末尾添加 ^boost ， 其中 boost 是一个浮点数：
  #+BEGIN_EXAMPLE
    {
        "multi_match": {
            "query":  "Quick brown fox",
            "fields": [ "*_title", "chapter_title^2" ] 
        }
    }
  #+END_EXAMPLE
  chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1
*** 跨字段搜索
有时我们想使用 单个 字符串在多个字段中进行搜索。比如存储的时候分了姓和名，但是在搜索时要搜索整个完整的名字。

有两种方法解决这个问题：
**** 自定义_all字段
Elasticsearch 在字段映射中为我们提供 copy_to 参数来实现这个功能：

#+BEGIN_EXAMPLE
  PUT /my_index
  {
      "mappings": {
          "person": {
              "properties": {
                  "first_name": {
                      "type":     "string",
                      "copy_to":  "full_name" 
                  },
                  "last_name": {
                      "type":     "string",
                      "copy_to":  "full_name" 
                  },
                  "full_name": {
                      "type":     "string"
                  }
              }
          }
      }
  }
#+END_EXAMPLE

注意： copy_to 设置对multi-field无效。如果尝试这样配置映射，Elasticsearch 会抛异常。

因为多字段只是以不同方式简单索引“主”字段；它们没有自己的数据源。也就是说没有可供 copy_to 到另一字段的数据源。

**** cross-fields查询
cross_fields 使用词中心式（term-centric）的查询方式，这与 best_fields 和 most_fields 使用字段中心式（field-centric）的查询方式非常不同，它将所有字段当成一个大字段，并在 每个字段 中查找 每个词 。

采用 cross_fields 查询与 自定义 _all 字段 相比，其中一个优势就是它可以在搜索时为单个字段提升权重。
#+BEGIN_EXAMPLE
  GET /books/_search
  {
      "query": {
          "multi_match": {
              "query":       "peter smith",
              "type":        "cross_fields",
              "fields":      [ "title^2", "description" ] 
          }
      }
  }
#+END_EXAMPLE

*** 部分匹配
但如果想匹配部分而不是全部的词该怎么办？ 部分匹配 允许用户指定查找词的一部分并找出所有包含这部分片段的词。

在某些情况下部分匹配会比较有用， 常见的应用如下：

+ 匹配邮编、产品序列号或其他 not_analyzed 未分析值，这些值可以是以某个特定前缀开始，也可以是与某种模式匹配的，甚至可以是与某个正则式相匹配的。
+ 输入即搜索（search-as-you-type） ——在用户键入搜索词过程的同时就呈现最可能的结果。
+ 匹配如德语或荷兰语这样有长组合词的语言，如： Weltgesundheitsorganisation （世界卫生组织，英文 World Health Organization）。

注意: prefix 、 wildcard 和 regexp 查询是基于词操作的，如果用它们来查询 analyzed 字段，它们会检查字段里面的每个词，而不是将字段作为整体来处理。

**** prefix前缀查询
prefix 查询是一个词级别的底层的查询，它不会在搜索之前分析查询字符串，它假定传入前缀就正是要查找的前缀。

#+BEGIN_EXAMPLE
  GET /my_index/address/_search
  {
      "query": {
          "prefix": {
              "postcode": "W1"
          }
      }
  }
#+END_EXAMPLE

默认状态下， prefix 查询不做相关度评分计算，它只是将所有匹配的文档返回，并为每条结果赋予评分值 1 。它的行为更像是过滤器而不是查询。 prefix 查询和 prefix 过滤器这两者实际的区别就是过滤器是可以被缓存的，而查询不行。

**** wildcard通配符查询
wildard查询使用标准的 shell 通配符查询： ? 匹配任意字符， * 匹配 0 或多个字符。
#+BEGIN_EXAMPLE
  GET /my_index/address/_search
  {
      "query": {
          "wildcard": {
              "postcode": "W?F*HW" 
          }
      }
  }
#+END_EXAMPLE

**** regexp正则表达式查询
#+BEGIN_EXAMPLE
  GET /my_index/address/_search
  {
      "query": {
          "regexp": {
              "postcode": "W[0-9].+" 
          }
      }
  }
#+END_EXAMPLE

*** boosting查询
可以通过boosting查询明确指定哪些关键字要提升权重,哪些关键字要降低权重
#+BEGIN_EXAMPLE
  GET /_search
  {
    "query": {
      "boosting": {
        "positive": {
          "match": {
            "text": "apple"
          }
        },
        "negative": {
          "match": {
            "text": "pie tart fruit crumble tree"
          }
        },
        "negative_boost": 0.5
      }
    }
  }
#+END_EXAMPLE
它接受 positive 和 negative 查询。只有那些匹配 positive 查询的文档罗列出来，对于那些同时还匹配 negative 查询的文档将通过文档的原始 _score 与 negative_boost 相乘的方式降级后的结果。

为了达到效果， negative_boost 的值必须小于 1.0 。

*** constant_score查询
有时候我们根本不关心 TF/IDF ， 只想知道一个词是否在某个字段中出现过。

在 constant_score 查询中，它可以包含查询或过滤，为任意一个匹配的文档指定评分 1 ，忽略 TF/IDF 信息：
#+BEGIN_EXAMPLE
  GET /_search
  {
    "query": {
      "bool": {
        "should": [
          { "constant_score": {
            "query": { "match": { "description": "wifi" }}
          }},
          { "constant_score": {
            "query": { "match": { "description": "garden" }}
          }},
          { "constant_score": {
            "boost":   2 
            "query": { "match": { "description": "pool" }}
          }}
        ]
      }
    }
  }
#+END_EXAMPLE

*** function_socre查询
function_score 查询 是用来控制评分过程的终极武器，它允许为每个与主查询匹配的文档应用一个函数， 以达到改变甚至完全替换原始查询评分 _score 的目的。

Elasticsearch 预定义了一些函数：

+ weight :: 为每个文档应用一个简单而不被规范化的权重提升值：当 weight 为 2 时，最终结果为 2 * _score 。 
+ field_value_factor :: 使用这个值来修改 _score ，如将 popularity 或 votes （受欢迎或赞）作为考虑因素。 
+ random_score :: 为每个用户都使用一个不同的随机评分对结果排序，但对某一具体用户来说，看到的顺序始终是一致的。 
+ 衰减函数 —— linear 、 exp 、 gauss :: 将浮动值结合到评分 _score 中，例如结合 publish_date 获得最近发布的文档，结合 geo_location 获得更接近某个具体经纬度（lat/lon）地点的文档，结合 price 获得更接近某个特定价格的文档。 
+ script_score :: 如果需求超出以上范围时，用自定义脚本可以完全控制评分计算，实现所需逻辑。 
                  
下面是个例子:
#+BEGIN_EXAMPLE
  GET /blogposts/post/_search
  {
    "query": {
      "function_score": {
        "query": {
          "multi_match": {
            "query":    "popularity",
            "fields": [ "title", "content" ]
          }
        },
        "field_value_factor": {
          "field":    "votes",
          "modifier": "log1p",
          "factor":   0.1
        },
        "boost_mode": "sum",
        "max_boost":  1.5 
      }
    }
  }
#+END_EXAMPLE

其中 ="field": "votes"= 说明根据votes字段中的值对结果进行调整

="modifier": "log1p"= 则指明了调整的方式. "log1p" 表示调整因子的计算方式为 =log(1 + number_of_votes)=
其他常用的值还包括 =none= （默认状态）、 =log= 、 =log1p= 、 =log2p= 、 =ln= 、 =ln1p= 、 =ln2p= 、 =square= 、 =sqrt= 以及 =reciprocal=

="boost_mode"= 则控制函数与原查询评分 _score 合并后的结果，参数接受的值为:
+ multiply :: 评分 _score 与函数值的积（默认） 
+ sum :: 评分 _score 与函数值的和 
+ min :: 评分 _score 与函数值间的较小值 
+ max :: 评分 _score 与函数值间的较大值 
+ replace :: 函数值替代评分 _score 
             
因此最终结果的计算公式为 =new_score = old_score + log(1 + 0.1 * number_of_votes)=

最后, ="max_boost"= 限定函数的最大限制


下面是另一个例子:
#+BEGIN_EXAMPLE
  GET /_search
  {
    "query": {
      "function_score": {
        "filter": { 
          "term": { "city": "Barcelona" }
        },
        "functions": [ 
          {
            "filter": { "term": { "features": "wifi" }}, 
            "weight": 1
          },
          {
            "filter": { "term": { "features": "garden" }}, 
            "weight": 1
          },
          {
            "filter": { "term": { "features": "pool" }}, 
            "weight": 2 
          }
          {
            "random_score": { 
              "seed":  "the users session id" 
            }
          }
        ],
        "score_mode": "sum", 
      }
    }
  }
#+END_EXAMPLE

首先要注意的是 =filter= 过滤器代替了 query 查询，function_score 查询接受 query 或 filter ，如果没有特别指定，则默认使用 match_all 查询。

functions 关键字保持着一个将要被使用的函数列表。 可以为列表里的每个函数都指定一个 filter 过滤器，在这种情况下，函数只会被应用到那些与过滤器匹配的文档，例子中，我们为与过滤器匹配的文档指定权重值 weight 为 1 （为与 pool 匹配的文档指定权重值为 2 ）。
其中 =random_score=  函数会输出一个 0 到 1 之间的数， 当种子 seed 值相同时，生成的随机结果是一致的


每个函数返回一个结果，所以需要一种将多个结果缩减到单个值的方式，然后才能将其与原始评分 _score 合并。评分模式 =score_mode= 参数正好扮演这样的角色， 它接受以下值：

+ multiply :: 函数结果求积（默认）。 
+ sum :: 函数结果求和。 
+ avg :: 函数结果的平均值。 
+ max :: 函数结果的最大值。 
+ min :: 函数结果的最小值。 
+ first :: 使用首个函数（可以有过滤器，也可能没有）的结果作为最终结果 
           

使用 =script_score= 则可以内嵌自己的Groovy代码来自定义计算函数:
#+BEGIN_EXAMPLE
  GET /_search
  {
    "function_score": {
      "functions": [
        {
          "script_score": {
            "params": { 
              "threshold": 80,
              "discount": 0.1,
              "target": 10
            },
            "script": "price  = doc['price'].value; margin = doc['margin'].value;
            if (price < threshold) { return price * margin / target };
            return price * (1 - discount) * margin / target;" 
          }
        }
      ]
    }
  }
#+END_EXAMPLE
** 模糊查询
*** fuzzy查询
fuzzy查询对比查询单词与被查询内容中单词的编辑距离来判断是否为同一个单词.

=fuzziness= 参数可以用于设置多少编辑距离内的单词被认为是同一个单词, 默认被设置为 AUTO ，这将导致以下的最大编辑距离：

+ 字符串只有 1 到 2 个字符时是 0
+ 字符串有 3 、 4 或者 5 个字符时是 1
+ 字符串大于 5 个字符时是 2 
  
例如,假设我们有以下文档:
#+BEGIN_SRC es
  POST /my_index/my_type/_bulk
  { "index": { "_id": 1 }}
  { "text": "Surprise me!"}
  { "index": { "_id": 2 }}
  { "text": "That was surprising."}
  { "index": { "_id": 3 }}
  { "text": "I wasn't surprised."}
#+END_SRC

那么我们执行
#+BEGIN_SRC es
  GET /my_index/my_type/_search
  {
    "query": {
      "fuzzy": {
        "text": {
          "value": "surprize",
          "fuzziness": 1
        }
      }
    }
  }
#+END_SRC

只能匹配 =Superise me!=

为了提高性能,你还可以通过 =prefix_length= 参数指定不能被 “模糊化” 的初始字符数,毕竟大部分的拼写错误发生在词的结尾，而不是词的开始。 

而 =max_expansions= 可以用来限制将产生的模糊选项的总数量
*** match查询的模糊匹配
通过设置match查询的 =fuzziness= 参数,也能进行模糊查询
#+BEGIN_SRC es
  GET /my_index/my_type/_search
  {
    "query": {
      "match": {
        "text": {
          "query":     "SURPRIZE ME!",
          "fuzziness": "AUTO",
          "operator":  "and"
        }
      }
    }
  }
#+END_SRC

同样， multi_match 查询也 支持 fuzziness
#+BEGIN_SRC es
  GET /my_index/my_type/_search
  {
    "query": {
      "multi_match": {
        "fields":  [ "text", "title" ],
        "query":     "SURPRIZE ME!",
        "fuzziness": "AUTO"
      }
    }
  }
#+END_SRC
** 搜索结果排序
#+BEGIN_SRC es
  GET /index/type/_search
  {
    query语句,
    "sort":[
      {"$FIELD":{"order":"asc"}}
    ]
  }
#+END_SRC
** 查看score怎么算出来的
#+BEGIN_SRC es
  GET /index/type/_search?explain
  {
    $搜索DSL
  }
#+END_SRC
** 分析一个文档是如何匹配的
#+BEGIN_SRC es
  GET /index/type/$id/_explain
  {
    $搜索DSL
  }
#+END_SRC
** scroll搜索
使用scroll滚动搜索,可以先搜索一批数据,然后下次再搜索一批数据,以此类推,直到搜索出全部的数据来.

scroll搜索会在第一次搜索的时候,保存一个当时的视图快照,之后基于该旧的试图快照来滚动搜索,期间数据的变化不会让用户看到.

每次发送scroll请求,我们需要指定一个scroll参数,指定一个时间窗口,表示每次搜索应该在这个时间窗口内完成.
#+BEGIN_SRC es
  GET /type/index/_search?scroll=1m
  {
    "query":{
      "match_all":{}
    },
    "sort":["_doc"],
    "size":10
  }
#+END_SRC

scroll搜索的结果中有一个scroll_id,下一次发送scroll请求时,需要带上该id
#+BEGIN_SRC es 
  GET /_search
  {
    "scroll":"1m",
    "scroll_id":"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  }
#+END_SRC
这里不用指定index,type以及query的其他参数,scroll_id本身会保存这些上下文.
* alias
#+BEGIN_SRC es
  PUT /index/_alias/alias_index
#+END_SRC

#+BEGIN_SRC es
  POST /_alias
  {
    "actions":{
      {"remove":{"index":"old_index","alias":"alias_index"}},
      {"add":{"index":"new_index","alias":"alias_index"}}
    }
  }
#+END_SRC
* 聚合分析
** bucket与metric的概念
+ bucket :: 一个数据分组
+ metric :: 对一个数据分组进行的统计
** group by(使用terms聚合来对数据进行分组)
#+BEGIN_SRC es
  GET /cheshi/product/_search
  {
    "aggs":{
      "group_by_tags":{
        "terms":{"field":"tags"}
      }
    }
  }
#+END_SRC
其中"group_by_tags"是任意取的一个名字,且文本field的fielddata属性要设置为true
#+BEGIN_SRC es
  PUT /cheshi/_mapping/product
  {
    "properties":{
      "tags":{
        "type":"text",
        "fielddata":"true"
      }
    }
  }
#+END_SRC

聚合还能用一个 =size= 参数指定返回几条聚合中的原始数据
#+BEGIN_SRC es
  GET /cheshi/product/_search
  {
    "size":0,
    "aggs":{
      "group_by_tags":{
        "terms":{"field":"tags"}
      }
    }
  }
#+END_SRC

** 先分组,再算平均值/最大值/最小值/总和(使用avg聚合对bucket进行metric)
#+BEGIN_SRC es
  GET /cheshi/product/_search
  {
    "size":0,
    "aggs":{
      "group_by_tags":{
        "terms":{"field":"tags"},
        "aggs":{
          "avg_price":{"avg":{"field":"price"},
          "min_price":{"min":{"field":"price"},
          "max_price":{"max":{"field":"price"},
          "sum_price":{"sum":{"field":"price"}
          }
        }
      }
    }
  }
#+END_SRC

** 下转分析(对一个bucket再分组)
就是在一个terms aggs下再嵌套一个terms aggs
#+BEGIN_SRC es
  GET /cheshi/product/_search
  {
    "size":0,
    "aggs":{
      "group_by_tags":{
        "terms":{"field":"tags"},
        "aggs":{
          "group_by_branch":{
            "terms":{"field":"branch"},
            "aggs":{
              "branch_avg_price":{
                "avg":{"field":"price"}
              }
            }
          }
        }
      }
    }
  }
#+END_SRC
** 分组排序
#+BEGIN_SRC es
  GET /cheshi/product/_search
  {
    "aggs":{
      "group_by_tags":{
        "terms":{
          "field":"tags",
          "order":{
            "avg_price":"desc"    // 按照avg_price排序
          }
        },
        "aggs":{
          "avg_price":{
            "avg":{
              "field":"price"
            }
          }
        }
      }
    }
  }
#+END_SRC
** histogram
类似terms,也是进行bucket分组操作,但是它接受一个field,然后按照field值的各个范围区间来进行分组(相比之下,terms是每个值一个分组)
#+BEGIN_SRC es
  GET /index/type/_search
  {
    "size":0,
    "aggs":{
      "prince":{
        "histogram":{
          "field":"price",
          "interval":20000
        },
        "aggs":{
          "revenue":{
            "sum":{"field":"price"}
          }
        }
      }
    }
  }
#+END_SRC
** date histogram
按照指定的某个date类型的日期field,以及日期的interval分组
#+BEGIN_SRC es
  GET /tvs/sales/_search
  {
    "size":0,
    "aggs":{
      "sales":{
        "date_histogram":{
          "field":"sold_date",
          "interval":"month",
          "format":"yyyy-MM-dd",
          "min_doc_count":0,      // 即使区间内没有记录,这个区间也需要返回
          "extended_bounds":{
            "min":"2017-01-01",
            "max":"2017-12-31"
          }
        }
      }
    }
  }
#+END_SRC
* 处理人类语言
以下列出了一些可优化的地方：

+ 清除类似 ´ ， ^ ， ¨ 的变音符号，这样在搜索 rôle 的时候也会匹配 role ，反之亦然。请见 归一化词元。
+ 通过提取单词的词干，清除单数和复数之间的差异—fox 与 foxes—以及时态上的差异—jumping 、 jumped 与 jumps 。请见 将单词还原为词根。
+ 清除常用词或者 停用词 ，如 the ， and ， 和 or ，从而提升搜索性能。请见 停用词: 性能与精度。
+ 包含同义词，这样在搜索 quick 时也可以匹配 fast ，或者在搜索 UK 时匹配 United Kingdom 。 请见 同义词。
+ 检查拼写错误和替代拼写方式，或者 同音异型词 —发音一致的不同单词，例如 their 与 there ， meat 、 meet 与 mete 。 请见 拼写错误。 
  
** 配置语言分析器
语言分析器都不需要任何配置，开箱即用， 它们中的大多数都允许你控制它们的各方面行为，具体来说：

+ 指定哪些词语不需要进行词干排除
+ 自定义哪些词作为停用词从分析列表中删除掉.
  
#+BEGIN_EXAMPLE
  PUT /my_index
  {
    "settings": {
      "analysis": {
        "analyzer": {
          "my_english": {
            "type": "english",
            "stem_exclusion": [ "organization", "organizations" ], 
            "stopwords": [ 
              "a", "an", "and", "are", "as", "at", "be", "but", "by", "for",
              "if", "in", "into", "is", "it", "of", "on", "or", "such", "that",
              "the", "their", "then", "there", "these", "they", "this", "to",
              "was", "will", "with"
            ]
          }
        }
      }
    }
  }
#+END_EXAMPLE

** ICU插件
standard 分词器是大多数语言分词的一个合理的起点，特别是西方语言。它也支持亚洲语言，只是有些缺陷，你可以考虑通过 ICU 插件的方式使用 icu_tokenizer 进行替换。

** 停用词

*** 指定停用词
为了让 标准分析器能与 自定义停用词表连用，我们要做的只需创建一个分析器的配置好的版本，然后将停用词列表传入：

#+BEGIN_EXAMPLE
  PUT /my_index
  {
    "settings": {
      "analysis": {
        "analyzer": {
          "my_analyzer": { 
            "type": "standard", 
            "stopwords": [ "and", "the" ] 
          }
        }
      }
    }
  }
#+END_EXAMPLE

+ 自定义的分析器名称为 my_analyzer 。
+ 这个分析器是一个标准 standard 分析器，进行了一些自定义配置。
+ 过滤掉的停用词包括 and 和 the 。 
  
特定语言的默认停用词，可以通过使用 _lang_ 符号来指定:
#+BEGIN_EXAMPLE
  "stopwords": "_english_"
#+END_EXAMPLE

停用词可以通过指定一个特殊列表 _none_ 来禁用。例如，使用 _english_ 分析器而不使用停用词，可以通过以下方式做到：
#+BEGIN_EXAMPLE
  PUT /my_index
  {
    "settings": {
      "analysis": {
        "analyzer": {
          "my_english": {
            "type":      "english", 
            "stopwords": "_none_" 
          }
        }
      }
    }
  }
#+END_EXAMPLE

最后，停用词还可以使用一行一个单词的格式保存在文件中。此文件必须在集群的所有节点上，并且通过 stopwords_path 参数设置路径:
#+BEGIN_EXAMPLE
  PUT /my_index
  {
    "settings": {
      "analysis": {
        "analyzer": {
          "my_english": {
            "type":           "english",
            "stopwords_path": "stopwords/english.txt" 
          }
        }
      }
    }
  }
#+END_EXAMPLE
+ 停用词文件的路径，该路径相对于 Elasticsearch 的 config 目录。

*** 根据频率自动识别停用词
match 查询接受一个参数 cutoff_frequency ，从而可以让它将查询字符串里的词项分为低频和高频两组。

#+BEGIN_EXAMPLE
  {
    "match": {
      "text": {
        "query": "Quick and the dead",
        "cutoff_frequency": 0.01 
      }
  }
#+END_EXAMPLE

+ 任何词项出现在文档中超过1%，被认为是高频词。cutoff_frequency 配置可以指定为一个分数（ 0.01 ）或者一个正整数（ 5 ）。 
+ 低频组（更重要的词项）组成 bulk 大量查询条件，而高频组（次重要的词项）只会用来评分，而不参与匹配过程。
  然后，此查询会被重写为以下的 bool 查询：
  #+BEGIN_EXAMPLE
    {
      "bool": {
        "must": { 
          "bool": {
            "should": [
              { "term": { "text": "quick" }},
              { "term": { "text": "dead"  }}
            ]
          }
        },
        "should": { 
          "bool": {
            "should": [
              { "term": { "text": "and" }},
              { "term": { "text": "the" }}
            ]
          }
        }
      }
    }
  #+END_EXAMPLE

*** 

* 备份与还原

** 备份
在备份前先需要创建一个保存数据的仓库,有多个仓库类型可以供你选择：
+ 共享文件系统，比如 NAS
+ Amazon S3
+ HDFS (Hadoop 分布式文件系统)
+ Azure Cloud 
  
*** 创建仓库
#+BEGIN_SRC es
  PUT _snapshot/my_backup 
  {
      "type": "fs", 
      "settings": {
        "location": "~/backups/es/my_backup",
        "max_snapshot_bytes_per_sec" : "50mb",
        "max_restore_bytes_per_sec" : "50mb"
      }
  }
#+END_SRC

其中
+ 仓库名为my_backup
+ =type= 指明了仓库类型为共享文件系统
+ =max_snapshot_bytes_per_sec= 控制了备份数据进仓库时的速度,默认为20mb/s
+ =max_restore_bytes_per_sec= 控制了恢复数据时的速度,默认也是20mb/s
+ 将request method从PUT改成POST会更改已有仓库的设置

*** 备份快照进仓库
一个仓库可以包含多个快照。 每个快照跟一系列索引相关（比如所有索引，一部分索引，或者单个索引）。当创建快照的时候，你指定你感兴趣的索引然后给快照取一个唯一的名字。
#+BEGIN_SRC es
  PUT _snapshot/my_backup/snapshot_2
  {
      "indices": "index_1,index_2"
  }
#+END_SRC
+ =indices= 指明了快照备份哪些索引,若省略则会备份所有的索引
+ 这个调用会立刻返回，然后快照会在后台运行。若希望调用等待快照完成后再返回,则需要添加 =wait_for_completion=true= 参数.

*** 获取快照信息
要获得单个快照的信息，直接对仓库和快照名发起一个 GET 请求：
#+BEGIN_SRC es
  GET _snapshot/my_backup/snapshot_2
#+END_SRC

更好的方案是拽取 _status API 数据：
#+BEGIN_SRC es
  GET _snapshot/my_backup/snapshot_3/_status
#+END_SRC
_status API 立刻返回，然后给出详细的多的统计值输出

要获取一个仓库中所有快照的完整列表，使用 _all 占位符替换掉具体的快照名称：
#+BEGIN_SRC es
  GET _snapshot/my_backup/_all
#+END_SRC

*** 删除快照
们需要一个命令来删除所有不再有用的旧快照 。这只要对仓库/快照名称发一个简单的 DELETE HTTP 调用：

#+BEGIN_SRC es
  DELETE _snapshot/my_backup/snapshot_2
#+END_SRC

*** 取消快照
你可能想取消一个快照或恢复。 因为它们是长期运行的进程，执行操作的时候一个笔误或者过错就会花很长时间来解决——而且同时还会耗尽有价值的资源。

要取消一个快照，在他进行中的时候简单的删除快照就可以：
#+BEGIN_SRC es
  DELETE _snapshot/my_backup/snapshot_3
#+END_SRC
这个会中断快照进程。然后删除仓库里进行到一半的快照。

** 恢复
*** 从快照恢复
一旦你备份过了数据，恢复它就简单了：只要在你希望恢复回集群的快照 ID 后面加上 _restore 即可：

#+BEGIN_SRC es
  POST /_snapshot/my_backup/snapshot_1/_restore
  {
      "indices": "index_1",
      "rename_pattern": "index_(.+)",
      "rename_replacement": "restored_index_$1"
  }
#+END_SRC
+ =indices= 指明了恢复哪些索引,默认恢复所有索引.
+ =rename_pattern= 和 =rename_replacement= 指明了如何重命名快照中的索引
+ restore 命令也会立刻返回，恢复进程会在后台进行。如果你更希望你的 HTTP 调用阻塞直到恢复完成，添加 wait_for_completion 标记：
  #+BEGIN_SRC es
    POST _snapshot/my_backup/snapshot_1/_restore?wait_for_completion
  #+END_SRC
+ 在内部实现上，从仓库恢复分片和从另一个节点恢复是等价的。
+ 如果你想监控恢复的进度，你可以使用 recovery API。这是一个通用目的的 API，用来展示你集群中移动着的分片状态。
  #+BEGIN_SRC es
    GET restored_index_3/_recovery
  #+END_SRC
*** 取消恢复
要取消一个恢复，你需要删除正在恢复的索引。 因为恢复进程其实就是分片恢复，发送一个 删除索引 API 修改集群状态，就可以停止恢复进程。比如：

#+BEGIN_SRC es
  DELETE /restored_index_3
#+END_SRC


* 中文分词
** ik插件
*** 安装
1. 去https://github.com/medcl/elasticsearch-analysis-ik/releases 下载对应版本的插件zip包
2. 将zip包中的内容解压到 elasticsearch中的 plugins/ik/ 目录中
3. 重启elasticsearch
*** 使用
ik提供了两种分析起 =ik_max_word= 和 =ik_smart= 我们可以看看效果
#+BEGIN_SRC es
  GET /_analyze
  {
    "text":"你会唱中华人民共和国国歌吗?",
    "analyzer":"ik_max_word"
  }
#+END_SRC


#+BEGIN_SRC es
  GET /_analyze
  {
    "text":"你会唱中华人民共和国国歌吗?",
    "analyzer":"ik_smart"
  }
#+END_SRC
*** 配置
+ IKAnalyzer.cfg.xml :: 用来配置自定义词库

+ main.dic :: ik原生内置的中文词库

+ quantifier.dic :: 表示单位的词

+ suffix.dic :: 后缀名

+ surname.dic :: 姓氏

+ stopword.dic :: 英文停用词
