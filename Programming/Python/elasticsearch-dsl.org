#+TITLE: elasticsearch-dsl
#+AUTHOR: lujun9972
#+TAGS: Programming, Python
#+DATE: [2017-06-05 一 11:38]
#+LANGUAGE:  zh-CN
#+OPTIONS:  H:6 num:nil toc:t \n:nil ::t |:t ^:nil -:nil f:t *:t <:nil

* Search DSL

** Search对象
我们使用Search对象来进行各种类型的搜索：
+ queries
+ filters
+ aggregations
+ sort
+ pagination
+ additional parameters
+ associated client

Search对象是不可边的，几乎所有的操作（除了aggregations）都会返回一个应用了该操作的Search对象 *副本*

当创建Search对象时，可以为 =using= 参数传递一个elasticsearch client来指定搜索哪个集群。
#+BEGIN_SRC python
  from elasticsearch import Elasticsearch
  from elasticsearch_dsl import Search

  client = Elasticsearch()

  s = Search(using=client)
#+END_SRC

不过你也可以调用 =using= 方法在后期修改Search对象的搜索集群：
#+BEGIN_SRC python
  s = s.using(client)
#+END_SRC

需要调用 =execute= 方法来真正发送请求到Elasticsearch:
#+BEGIN_SRC python
  s = Search().using(client).query("match", title="python")
  response = s.execute()
#+END_SRC

返回的查询结果可以直接遍历
#+BEGIN_SRC python
  for hit in s:
      print(hit.title)
#+END_SRC

需要注意的是: Search的结果会被缓存下来,这样下次再调用该Search对象的execute方法时并不会再此申请查询.
要强制重新查询需要在调用 =execute= 方法时指定 =ignore_cache=True=.

为了方便调试, Search对象还提供了 =to_dict()= 方法,可以将查询条件用字典的方式显示出来.

*** Delete By Query
通过调用Search对象的 =delete= 方法可以删除掉符合条件的文档.
#+BEGIN_SRC python
  s = Search().query("match", title="python")
  response = s.delete()
#+END_SRC

*** 各种类型的查询类
该库为所有类型的查询都设计了独立的类. 这些类可以接受任意的关键字参数,这些关键字会成为最终请求报文的顶层key.
#+BEGIN_SRC python
  from elasticsearch_dsl.query import MultiMatch, Match

  # {"multi_match": {"query": "python django", "fields": ["title", "body"]}}
  MultiMatch(query='python django', fields=['title', 'body'])

  # {"match": {"title": {"query": "web framework", "type": "phrase"}}}
  Match(title={"query": "web framework", "type": "phrase"})
#+END_SRC

你也可以统一使用 =Q= 来生成这些查询对象
#+BEGIN_SRC python
  Q("multi_match", query='python django', fields=['title', 'body'])
  Q({"multi_match": {"query": "python django", "fields": ["title", "body"]}})
#+END_SRC

=Q= 的参数还可以是其他查询对象:
#+BEGIN_SRC python
  q = Q('bool',
      must=[Q('match', title='python')],
      should=[Q(...), Q(...)],
      minimum_should_match=1
  )
  s = Search().query(q)
#+END_SRC

Search对象的query方法也可以直接接受一个查询对象 #+BEGIN_SRC python
  q = Q("multi_match", query='python django', fields=['title', 'body'])
  s = s.query(q)
#+END_SRC

*** Query combination
可以使用逻辑操作符整合多个查询对象:
#+BEGIN_SRC python
  Q("match", title='python') | Q("match", title='django')
  # {"bool": {"should": [...]}}

  Q("match", title='python') & Q("match", title='django')
  # {"bool": {"must": [...]}}

  ~Q("match", title="python")
  # {"bool": {"must_not": [...]}}
#+END_SRC

当对一个Search对象多次调用query方法,则表示使用 =&= 来整合这些查询条件.

*** Filters
若希望在filter context中进行查询,可以使用Search对象的 =filter= 方法:
#+BEGIN_SRC python
  s = Search()
  s = s.filter('terms', tags=['search', 'python'])
  # 等价于
  s = Search()
  s = s.query('bool', filter=[Q('terms', tags=['search', 'python'])])
#+END_SRC

也可以使用 =exclude= 方法排除某些item
#+BEGIN_SRC python
  s = Search()
  s = s.exclude('terms', tags=['search', 'python'])
#+END_SRC

*** Aggregations
To define an aggregation, you can use the A shortcut:

#+BEGIN_SRC python
  A('terms', field='tags')
  # {"terms": {"field": "tags"}}
#+END_SRC

To nest aggregations, you can use the .bucket(), .metric() and .pipeline() methods:

#+BEGIN_SRC python
  a = A('terms', field='category')
  # {'terms': {'field': 'category'}}

  a.metric('clicks_per_category', 'sum', field='clicks')\
      .bucket('tags_per_category', 'terms', field='tags')
  # {
  #   'terms': {'field': 'category'},
  #   'aggs': {
  #     'clicks_per_category': {'sum': {'field': 'clicks'}},
  #     'tags_per_category': {'terms': {'field': 'tags'}}
  #   }
  # }
#+END_SRC

To add aggregations to the Search object, use the .aggs property, which acts as a top-level aggregation:

#+BEGIN_SRC python
  s = Search()
  a = A('terms', field='category')
  s.aggs.bucket('category_terms', a)
  # {
  #   'aggs': {
  #     'category_terms': {
  #       'terms': {
  #         'field': 'category'
  #       }
  #     }
  #   }
  # }
#+END_SRC

or

#+BEGIN_SRC python
  s = Search()
  s.aggs.bucket('articles_per_day', 'date_histogram', field='publish_date', interval='day')\
      .metric('clicks_per_day', 'sum', field='clicks')\
      .pipeline('moving_click_average', 'moving_avg', buckets_path='clicks_per_day')\
      .bucket('tags_per_day', 'terms', field='tags')

  s.to_dict()
  # {
  #   "aggs": {
  #     "articles_per_day": {
  #       "date_histogram": { "interval": "day", "field": "publish_date" },
  #       "aggs": {
  #         "clicks_per_day": { "sum": { "field": "clicks" } },
  #         "moving_click_average": { "moving_avg": { "buckets_path": "clicks_per_day" } },
  #         "tags_per_day": { "terms": { "field": "tags" } }
  #       }
  #     }
  #   }
  # }
#+END_SRC

You can access an existing bucket by its name:

#+BEGIN_SRC python
  s = Search()

  s.aggs.bucket('per_category', 'terms', field='category')
  s.aggs['per_category'].metric('clicks_per_category', 'sum', field='clicks')
  s.aggs['per_category'].bucket('tags_per_category', 'terms', field='tags')
#+END_SRC

*** Sorting
To specify sorting order, use the .sort() method:

#+BEGIN_SRC python
  s = Search().sort(
      'category',
      '-title',
      {"lines" : {"order" : "asc", "mode" : "avg"}}
  )
#+END_SRC

It accepts positional arguments which can be either strings or dictionaries. String value is a field name, optionally prefixed by the - sign to specify a descending order.

To reset the sorting, just call the method with no arguments:

#+BEGIN_SRC python
  s = s.sort()
#+END_SRC

*** 分页
分页的方法就算是使用Python的切片
#+BEGIN_SRC python
  s = s[10:20]
  # {"from": 10, "size": 10}
#+END_SRC

*** Highlighting

To set common attributes for highlighting use the highlight_options method:

#+BEGIN_SRC python
  s = s.highlight_options(order='score')
#+END_SRC

Enabling highlighting for individual fields is done using the highlight method:

#+BEGIN_SRC python
  s = s.highlight('title')
  # or, including parameters:
  s = s.highlight('title', fragment_size=50)
#+END_SRC

The fragments in the response will then be available on reach Result object as .meta.highlight.FIELD which will contain the list of fragments:

#+BEGIN_SRC python
  response = s.execute()
  for hit in response:
      for fragment in hit.meta.highlight.title:
          print(fragment)
#+END_SRC

*** Suggestions
To specify a suggest request on your Search object use the suggest method:

#+BEGIN_SRC python
  s = s.suggest('my_suggestion', 'pyhton', term={'field': 'title'})
#+END_SRC

The first argument is the name of the suggestions (name under which it will be returned), second is the actual text you wish the suggester to work on and the keyword arguments will be added to the suggest’s json as-is which means that it should be one of term, phrase or completion to indicate which type of suggester should be used.

If you only wish to run the suggestion part of the search (via the _suggest endpoint) you can do so via execute_suggest:

#+BEGIN_SRC python
  s = s.suggest('my_suggestion', 'pyhton', term={'field': 'title'})
  suggestions = s.execute_suggest()

  print(suggestions.my_suggestion)
#+END_SRC

*** Extra properties and parameters

To set extra properties of the search request, use the .extra() method. This can be used to define keys in the body that cannot be defined via a specific API method like explain or search_after:

#+BEGIN_SRC python
  s = s.extra(explain=True)
#+END_SRC

To set query parameters, use the .params() method:

#+BEGIN_SRC python
  s = s.params(search_type="count")
#+END_SRC

If you need to limit the fields being returned by elasticsearch, use the source() method:

#+BEGIN_SRC python
  # only return the selected fields
  s = s.source(['title', 'body'])
  # don't return any fields, just the metadata
  s = s.source(False)
  # explicitly include/exclude fields
  s = s.source(include=["title"], exclude=["user.*"])
  # reset the field selection
  s = s.source(None)
#+END_SRC

*** 序列化与反序列化
调用Search对象的 =to_dict()= 方法可以将Search对象转换成字典格式.

反之,调用类方法 =from_dict= 可以根据字典生成心的Search对象.
#+BEGIN_SRC python
  s = Search.from_dict({"query": {"match": {"title": "python"}}})
#+END_SRC

也可以调用Search对象中的 =update_from_dict= 来更新本对象中的内容:
#+BEGIN_SRC python
  s = Search(index='i')
  s.update_from_dict({"query": {"match": {"title": "python"}}, "size": 42})
#+END_SRC

** Response对象
调用Rearch对象的 =execute= 方法后就会返回一个Response对象.

要获取回应中的内容也很简单,直接用点符号访问就行了:
#+BEGIN_SRC python
  response = s.execute()

  print(response.success())
  # True

  print(response.took)
  # 12

  print(response.hits.total)

  print(response.suggest.my_suggestions)
#+END_SRC

若想获取response中的元数据,则直接访问Response对象中的 =meta= 属性(注意不是_meta):
#+BEGIN_SRC python
  response = s.execute()
  h = response.hits[0]
  print('/%s/%s/%s returned with score %f' % (
      h.meta.index, h.meta.doc_type, h.meta.id, h.meta.score))
#+END_SRC

注意: *若你的文档中恰好有一个名叫 meta 的域,你只能用 hit['meta'] 这种方法来访问它.*

** MultiSearch
你可以使用MultiSearch类来一次性进行多个查询
#+BEGIN_SRC python
  from elasticsearch_dsl import MultiSearch, Search

  ms = MultiSearch(index='blogs')

  ms = ms.add(Search().filter('term', tags='python'))
  ms = ms.add(Search().filter('term', tags='elasticsearch'))

  responses = ms.execute()

  for response in responses:
      print("Results for query %r." % response.search.query)
      for hit in response:
          print(hit.title)
#+END_SRC

* Persistence

** 定义Mappings
#+BEGIN_SRC python
  from elasticsearch_dsl import Keyword, Mapping, Nested, Text

  # name your type
  m = Mapping('my-type')

  # add fields
  m.field('title', 'text')

  m.field('tags', Keyword(multi=True))

  # you can use multi-fields easily
  m.field('category', 'text', fields={'raw': Keyword()})

  # you can also create a field manually
  comment = Nested()
  comment.field('author', Text())
  comment.field('created_at', Date())

  # and attach it to the mapping
  m.field('comments', comment)

  # you can also define mappings for the meta fields
  m.meta('_all', enabled=False)

  # save the mapping into index 'my-index'
  m.save('my-index')
#+END_SRC

field函数支持两个keyword:
+ multi :: 设置为 True 则表示该域为列表
+ required :: 设置该field是否必须有值

** 定义分析器
#+BEGIN_SRC python
  from elasticsearch_dsl import analyzer, tokenizer

  my_analyzer = analyzer('my_analyzer',
      tokenizer=tokenizer('trigram', 'nGram', min_gram=3, max_gram=3),
      filter=['lowercase']
  )
#+END_SRC

** 文档操作
*** 定义文档类型
要定义自己的文档类型,可以让它继承至DocType类
#+BEGIN_SRC python
  from datetime import datetime
  from elasticsearch_dsl import DocType, Date, Nested, Boolean, \
      analyzer, InnerObjectWrapper, Completion, Keyword, Text

  html_strip = analyzer('html_strip',
      tokenizer="standard",
      filter=["standard", "lowercase", "stop", "snowball"],
      char_filter=["html_strip"]
  )

  class Comment(InnerObjectWrapper):
      def age(self):
          return datetime.now() - self.created_at

  class Post(DocType):
      title = Text()
      title_suggest = Completion()
      created_at = Date()
      published = Boolean()
      category = Text(
          analyzer=html_strip,
          fields={'raw': Keyword()}
      )

      comments = Nested(
          doc_class=Comment,
          properties={
              'author': Text(fields={'raw': Keyword()}),
              'content': Text(analyzer='snowball'),
              'created_at': Date()
          }
      )

      class Meta:
          index = 'blog'

      def add_comment(self, author, content):
          self.comments.append(
            {'author': author, 'content': content})

      def save(self, ** kwargs):
          self.created_at = datetime.now()
          return super().save(** kwargs)
#+END_SRC

这就创建了一个名为Post的文档类型. 但是在真正使用该文档类型之前,需要调用 =Post.init()= 来先创建mapping

*** 新增文档
要实际创建一个post,只需要实例化Post类,然后调用势利的 =save()= 方法即可.
#+BEGIN_SRC python
  # instantiate the document
  first = Post(title='My First Blog Post, yay!', published=True)
  # assign some field values, can be values or lists of values
  first.category = ['everything', 'nothing']
  # every document has an id in meta
  first.meta.id = 47

  # save the document into the cluster
  first.save()
#+END_SRC

要注意的是,新建的文档类型中的 =meta= 属性是保留下来专门访问 =_meta= 属性的,所以如果你的文档有 =meta= 这个字段,那么你需要通过 =post['meta']= 来访问.
*** 获取/更新已有文档
你还可以使用类方法 =Post.get= 来获取已存的文档
#+BEGIN_SRC python
  # retrieve the document
  first = Post.get(id=42)
  # now we can call methods, change fields, ...
  first.add_comment('me', 'This is nice!')
  # and save the changes into the cluster again
  first.save()

  # you can also update just individual fields which will call the update API
  # and also update the document in place
  first.update(published=True, published_by='me')
#+END_SRC

若找不到指定文档,则会跑出异常 =elasticsearch.NotFoundError=. 不过你可以通过设置 =ignore=404= 来让方法返回None
#+BEGIN_SRC python
  p = Post.get(id='not-in-es', ignore=404)
  p is None
#+END_SRC

你还可以使用类方法 =Post.mget= 来一次性获取多个文档
#+BEGIN_SRC python
  posts = Post.mget([42, 47, 256])
#+END_SRC
mget方法可能抛出 =NotFoundError= 和 =RequestError= 异常,不过这一行为可以通过下面参数进行修改

+ missing :: 有三个值:none(默认值),表示文档为None;raise,表示抛出异常;skip表示返回的结果根本不要包含该文档.
+ raise_on_error :: 默认为True,表示有异常的话,则抛出异常. 若为False,则有异常的文档当成missing来处理.
*** 删除文档
只需要调用对象方法中的 =delete()= 方法即可
#+BEGIN_SRC python
  first = Post.get(id=42)
  first.delete()
#+END_SRC
*** 搜索文档
可以通过调用类方法 =search= 来创建一个针对该文档类型的搜索器
#+BEGIN_SRC python
  # by calling .search we get back a standard Search object
  s = Post.search()
  # the search is already limited to the index and doc_type of our document
  s = s.filter('term', published=True).query('match', title='first')


  results = s.execute()

  # when you execute the search the results are wrapped in your document class (Post)
  for post in results:
      print(post.meta.score, post.title)
#+END_SRC
** Index类创建索引
#+BEGIN_SRC python
  from elasticsearch_dsl import Index, DocType, Text, analyzer

  blogs = Index('blogs')

  # define custom settings
  blogs.settings(
      number_of_shards=1,
      number_of_replicas=0
  )

  # define aliases
  blogs.aliases(
      old_blogs={}
  )

  # register a doc_type with the index
  blogs.doc_type(Post)

  # can also be used as class decorator when defining the DocType
  @blogs.doc_type
  class Post(DocType):
      title = Text()

  # You can attach custom analyzers to the index

  html_strip = analyzer('html_strip',
      tokenizer="standard",
      filter=["standard", "lowercase", "stop", "snowball"],
      char_filter=["html_strip"]
  )

  blogs.analyzer(html_strip)

  # delete the index, ignore if it doesn't exist
  blogs.delete(ignore=404)

  # create the index in elasticsearch
  blogs.create()
#+END_SRC

你还可以使用 =clone= 方法复制一份配置一样的索引
#+BEGIN_SRC python
  blogs = Index('blogs', using='production')
  blogs.settings(number_of_shards=2)
  blogs.doc_type(Post)

  # create a copy of the index with different name
  company_blogs = blogs.clone('company-blogs')

  # create a different copy on different cluster
  dev_blogs = blogs.clone('blogs', using='dev')
  # and change its settings
  dev_blogs.setting(number_of_shards=1)
#+END_SRC
